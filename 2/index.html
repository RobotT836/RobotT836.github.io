<!DOCTYPE html>

<html>

<head>
    <title>Project 2</title>
    <style>
        body {
            font-family: Arial, Helvetica, sans-serif;
            margin: 20px;
        }
    </style>
</head>

<body>
    <style>
      body {
        background-color: rgb(163, 168, 173); 
      }
    </style>

    <h1>Project 2</h1>
    <h2>Fun with Filters and Frequencies</h2>
    <div></div>
    <h3>Part 1.1</h3>
    <p>For this part I implemented two types of custom convolution functions: one naive function, and one numpy vectorized 
      function. I then compared the performance of each version with the scipy convolve2d function and found the library function to be vastly
      more efficient (as expected). Boundaries are handled using zero padding. Below are sample photos and snippets of the code used:
    </p>
    <h4>Code</h4>
    <p><pre><code>
#Naive Convolution
def myconvolve_naive(image, kernel):
    h_kern, w_kern = kernel.shape
    kernel = np.flip(kernel)

    #padding 
    pad_top = (h_kern - 1) // 2
    pad_bottom = (h_kern - 1) - pad_top
    pad_left = (w_kern - 1) // 2
    pad_right = (w_kern - 1) - pad_left
    paddingdims = ((pad_top, pad_bottom), (pad_left, pad_right))
    padded = np.pad(image, paddingdims, mode='constant', constant_values=0)
    h_im, w_im = padded.shape

    output = np.zeros((int(h_im - h_kern + 1), int(w_im - w_kern + 1)))
    h_out, w_out = output.shape

    for row in range(h_out):
        for col in range(w_out):

            outputval = 0
            
            for kernr in range(h_kern):
                for kernc in range(w_kern):
                    imval = padded[row + kernr, col + kernc]
                    kernval = kernel[kernr, kernc]
                    outputval += imval * kernval

            output[row, col] = outputval
    return output

#Optimized convoluton
def myconvolve(image, kernel):
    h_kern, w_kern = kernel.shape
    kernel = np.flip(kernel)

    #padding 
    pad_top = (h_kern - 1) // 2
    pad_bottom = (h_kern - 1) - pad_top
    pad_left = (w_kern - 1) // 2
    pad_right = (w_kern - 1) - pad_left
    paddingdims = ((pad_top, pad_bottom), (pad_left, pad_right))
    padded = np.pad(image, paddingdims, mode='constant', constant_values=0)
    h_im, w_im = padded.shape

    output = np.zeros((int(h_im - h_kern + 1), int(w_im - w_kern + 1)))
    h_out, w_out = output.shape

    for r in range(h_out):
        for c in range(w_out):
            rend = r + h_kern
            cend = c + w_kern

            output[r, c] = np.sum(padded[r:rend, c:cend] * kernel)

    return output
    </code></pre></p>
    <h4>Pictures</h4>
    <figure>
      <img src="./media/P1_Opt.png" width="360">
      <figcaption>Convolved Image using Optimized Implementation</figcaption>
    </figure>
    <br>
    <h4>Runtimes</h4>
    Naive: 17.15 seconds<br>
    Optimized: 14.37 seconds<br>
    Library: 0.11 Seconds<br>

    <h3>Part 1.2</h3>
    <p>For this part, I computed the directional derivatives as usual, then computed the gradient magnitude image by taking the L2 norm.
      Filtering all pixels below a certain threshold (in my case, 0.1) gave the edge image. This version, without any filters applied, did
      have a significant amount of noise mostly from the background.
      As a note for 1.2 and 1.3, I chose to not change edge values to emphasize the changes made by using the DoGs.
    </p>
    <figure>
      <img src="./media/P1.2_derivs.png", width="360">
      <figcaption>Derivatives</figcaption>
    </figure>
    <figure>
      <img src="./media/P1.2_gradmag.png", width="360">
      <figcaption>Gradient Magnitude</figcaption>
    </figure>
    <figure>
      <img src="./media/P1.2_edge.png", width="360">
      <figcaption>Edge Image</figcaption>
    </figure>

    <h3>Part 1.3</h3>
    <p>
      Like part 1.3, I performed the same steps as before, except this time before finding the directional derivatives I convoled
      with the 2D Gaussian to get the Derivative of Gaussian images. The resulting gradient magnitude and edge images looked a lot
      cleaner.
    </p>
    <figure>
      <img src="./media/P1.3_DoGs.png", width="360">
      <figcaption>Derivatives</figcaption>
    </figure>
    <figure>
      <img src="./media/P1.3_gradmag.png", width="360">
      <figcaption>Gradient Magnitude</figcaption>
    </figure>
    <figure>
      <img src="./media/P1.3_edge.png", width="360">
      <figcaption>Edge Image</figcaption>
    </figure>


    <h3>Part 2.1</h3>
    <p>
      To sharpen images I followed the steps outlined in class. First, I took the Gaussian of the original image and subtracted from 
      the original to obtain the high frequency "detail" image. Then I added the details to the original with a small alpha 
      to obtain the sharpened image.
      Below is an example using the provided image, and a few of my own!
    </p>
    <figure>
      <img src="./media/P2.1.png", width="480">
      <figcaption>Sharpening Example</figcaption>
    </figure>

    <h3>Part 2.2</h3>
    <p>
      To create hybrid images I added the Gaussian of one image and the Laplacian of another (I found the laplacian by performing 
      similar steps as Part 2.1). Adding the Guassian and Laplacian then creates the hybrid image. Below are examples on the example 
      image and some of my own examples.
    </p>
    <figure>
      <img src="./media/P2.2_ex1.png", width="480">
      <figcaption>Example 1: Derek and Nutmeg</figcaption>
    </figure>
    <figure>
      <img src="./media/P2.2_hybrid1.png", width="480">
      <figcaption>Resulting Image</figcaption>
    </figure>
    <figure>
      <img src="./media/P2.2_ftlog1.png", width="480">
      <figcaption>FT Log Magnitude</figcaption>
    </figure>
    <figure>
      <img src="./media/happy.jpg", width="480">
      <figcaption>Example 2: Happy</figcaption>
    </figure>
    <figure>
      <img src="./media/angry.jpg", width="480">
      <figcaption>Example 2: Angry</figcaption>
    </figure>

    <figure>
      <img src="./media/happyangry.png", width="480">
      <figcaption>Example 2: Faces</figcaption>
    </figure>

    <figure>
      <img src="./media/dog.jpg", width="480">
      <figcaption>Example 3: Dog</figcaption>
    </figure>

    <figure>
      <img src="./media/cat.jpg", width="480">
      <figcaption>Example 3:Cat</figcaption>
    </figure>

    <figure>
      <img src="./media/dogcat_hybrid.png", width="480">
      <figcaption>Example 3: Dog and Cat</figcaption>
    </figure>



    <h3>Part 2.3</h3>
    <p>To implement the stacks, I combined their functionality into one function (since you can't compute the Laplacian stack without the lowest
      frequency image of the Gaussian anyway). The Gaussian stack was made by repeatedly applying the Gaussian to an image without subsampling. Once
    the Gaussian stack was computed, I subtracted the levels of the Gaussian from the next highest level of the Gaussian stack to compute the high 
  frequency images for the Laplacian stack. Then I appended the lowest frequency image of the Gaussian stack to the Laplacian stack to ensure no 
information is lost. The result on the apple and orange looks almost exactly like the results from the paper: </p>

    <figure>
      <img src="./media/apple_stacks.png", width="480">
      <figcaption>Gaussian and Laplacian Stacks: Apple</figcaption>
    </figure>
    <figure>
      <img src="./media/orange_stacks.png", width="480">
      <figcaption>Gaussian and Laplacian Stacks: Orange</figcaption>
    </figure>



    <h3>Part 2.4</h3>
    <p>Finally, multiresolution blending. Using the results from 2.3 I then created a vertical seam mask like the one in the paper and applied the Gaussian
      filter to it. Using the formula from lecture, I combined the laplacians of both source images with the Gaussian of the mask to create the blended image.
    </p>
    <figure>
      <img src="./media/the_orapple.png", width="480">
      <figcaption>The Orapple</figcaption>
    </figure>
    <p>And a few examples of my own using different masks:</p>
    <figure>
      <img src="./media/handeye.png", width="480">
      <figcaption>Couldn't get proportions right, but terrifying nonetheless</figcaption>
    </figure>
    <figure>
      <img src="./media/urbrural.png", width="480">
      <figcaption>Kinda like how this one turned out, but a bit blurry</figcaption>
    </figure>
    
</body>