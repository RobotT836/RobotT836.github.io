<!DOCTYPE html>

<html>

<head>
    <title>Project 3</title>
    <style>
        body {
            font-family: Arial, Helvetica, sans-serif;
            margin: 20px;
        }
    </style>
</head>

<body>
    <style>
      body {
        background-color: rgb(163, 168, 173); 
      }
    </style>
    <a href="https://robott836.github.io/">Home</a>

    <h1>Project 3A: Image Warping and Mosaicing</h1>
    <h2></h2>
    <div></div>
    <h3>Part A.1</h3>
    <p>
        I chose scenes and landscape with relitively moderate amounts of detail that was far enough away from the center of projection.
        I found that having more detailed objects and having more objects closer to the camera made for bad sample photographs
        as, because of the limited amount of correspondences and the fact that they have to be chosen by hand, it's hard to accoutn for every
        detail which results in ghosting. All photos were taken with a wide angle lens th minimize barrel distortions and better capture the scene:
    </p>
    
    <figure>
      <img src="./media/M11.jpg" width="480">
      <figcaption>Mosaic Image 1.1</figcaption>
    </figure>
    <figure>
      <img src="./media/M12.jpg" width="480">
      <figcaption>Mosaic Image 1.2</figcaption>
    </figure>
    <figure>
      <img src="./media/M21.jpg" width="480">
      <figcaption>Mosaic Image 2.1</figcaption>
    </figure>
    <figure>
      <img src="./media/M22.jpg" width="480">
      <figcaption>Mosaic Image 2.2</figcaption>
    </figure>
    <figure>
      <img src="./media/M31.jpg" width="480">
      <figcaption>Mosaic Image 3.1</figcaption>
    </figure>
    <figure>
      <img src="./media/M32.jpg" width="480">
      <figcaption>Mosaic Image 3.2</figcaption>
    </figure>
    <br>


    <h3>Part A.2</h3>
    <p>
        Computing the homography between the two images was just a matter of rearranging. We are given correspondences (p, the points in the image
        to be transformed and p', the points of the reference/base image) and the homography matrix H as p' = H * p. We want to rearrange this into the 
        form Ax = b so that we can solve for the parameters of the homography matrix using least squares. In this case, our x would be h, the 1D flattened 
        homography matrix with the last value (scaling factor) set to 1. This gives us the following system of equations (and the following derivation for A and b):
    </p>
    <figure>
      <img src="./media/deriv.png", width="480">
      <figcaption>Derivation of the System</figcaption>
    </figure>

    <p>Also below are hand picked correspondences for each image pair</p>
    <figure>
      <img src="./media/correspondences_1.png", width="960">
      <figcaption>Pair 1</figcaption>
    </figure>
    <figure>
      <img src="./media/correspondences_2.png", width="960">
      <figcaption>Pair 2</figcaption>
    </figure>
    <figure>
      <img src="./media/correspondences_3.png", width="960">
      <figcaption>Pair 3</figcaption>
    </figure>

    <h3>Part A.3</h3>
    <p>
      To warp images, I performed both nearest neighbors and bilinear interpolation. Previous naive versions were exceptionally slow (anywhere from 1-2 minutes
      for nearest neighbors and up to 5 minutes for bilinear), so I decided to optimize both using numpy techniques. Overall though, bilinear takes longer than 
      nearest neighbors due to its complexity (sampling 4 neighbors and averaging instead of snapping the value to one neighbor). I did note that bilinear
      interpolation seemef to decrease the size of the final rectified image, possibly due to a larger number of pixels being sampled. Here are the example images
      and their rectified versions:
    </p>
    <figure>
      <img src="./media/cornerpts_r1.png", width="480">
      <figcaption>Original Image 1</figcaption>
    </figure>
    <figure>
      <img src="./media/rectified_r1.png", width="960">
      <figcaption>Rectified Image 1</figcaption>
    </figure>
    <figure>
      <img src="./media/cornerpts_r2.png", width="480">
      <figcaption>Original Image 2</figcaption>
    </figure>
    <figure>
      <img src="./media/rectified_r2.png", width="960">
      <figcaption>Rectified Image 2</figcaption>
    </figure>

    <h3>Part A.4</h3>
    <p>Finally, to blend the images into a mosaic, I decided to perform blending using a Laplacian Stack. As mentioned before, I had trouble with details in the 
        images causing ghosting, so I experimented with Laplacian stacks to see if that would help. As the fault was with the images, I decided to keep the
        stack implementation.

        To stitch together the final mosaic, I first had to determine the final bounds of the mosaic. This was done by warping the corners of the first image using the 
        computed homography matrix, then using those as upper/lower bounds for the final image. Next was to create the image masks to be used by the stack blending 
        implementation to get rid of artifacts, and initialize the final osaic with the second/reference image. I then used bilinear interpolation on image 1 and its mask
        to warp it into image 2's frame. 

        After warping image 1, I calcualted the area of the overlap between the two images. If there was any overlap at all, I had to change the mask to only include
        overlapped areas. I used the python version of the / operator from Matab that was mentioned in lecture to compute the boundary distance between images 1 and 2, 
        then applied the resulting value to the mask (after smoothing, to reduce harsh edges along the boundary in the final mosaic).

        After all this, it's finally time to blend. I call my stack implementation, which is an almost exact copy of my version from Project 2. It calculates a 
        gaussian and laplacian stack and applies smoothing over the boundary surface defined by the mask. The result is a smooth mosaic with very little visible artifacts and 
        harsh borders. Source images are shown above in A.1:
    </p>

    <figure>
      <img src="./media/mosaic1.png", width="600">
      <figcaption>Pair 1 Mosaic</figcaption>
    </figure>
    <figure>
      <img src="./media/mosaic2.png", width="600">
      <figcaption>Pair 2 Mosaic</figcaption>
    </figure>
    <figure>
      <img src="./media/mosaic3.png", width="600">
      <figcaption>Pair 3 Mosaic</figcaption>
    </figure>


    <h1>Project 3B: Feature Matching for Autostitching</h1>

    <h3>Part B.1</h3>
    <p>
      For Harris corner detection, I used the sample code provided by the project spec, as 
      implementing Harris corners from scratch is not trivial. I then tweaked the parameters until
      I had a nice amount of points to perform ANMS on.

      Following the ideas of the paper, I performed ANMS by using the distance squared metric of the 
      points as a corner strength score. I defined a mask for the radius and performed the minimization 
      outlined in the paper to only take into account local maxima. The result leads to a much cleaner 
      number and spread of Harris corners.
    </p>

    <figure>
      <img src="./media/B_orig11.png", width="600">
      <figcaption>Original Picture</figcaption>
    </figure>
    <figure>
      <img src="./media/B_harris11.png", width="600">
      <figcaption>Harris corners, max of 5000 peaks</figcaption>
    </figure>
    <figure>
      <img src="./media/B_anms11.png", width="600">
      <figcaption>ANMS applied, max corners = 500</figcaption>
    </figure>

    <h3>Part B.2</h3>
    <p>
      To calculate feature descriptors, I turned to the paper again and implemented MOPS.
      First, I defined a 40x40 window over each corner and convolved with the gaussian to 
      blur (since we have to sample pixels at a lower frequency according to the paper i.e. 
      convolve with the gaussian to blur). I then simply downsized the the 8x8 sample size and 
      normalized to obtain the feature vector at that corner.
    </p>

    <figure>
      <img src="./media/B_features11.png", width="600">
      <figcaption>Extracted Features from Image 1</figcaption>
    </figure>

    <h3>Part B.3</h3>
    <p>
      Next, I implemented feature matching using Lowe's technique, or the ratio test. 
      We can do this by comparing the ratio of the errors of the first and second
      nearest neighbors to a point. If the ratio is below out threshold, we have a possible
      correspondence. To get a good number of correspondences gained/left over for RANSAC,
      I had to set the threshold quite high, but RANSAC should take care of the outliers that
      result from this.
    </p>

    <figure>
      <img src="./media/B_corrs1.png", width="600">
      <figcaption>Extracted Correspondences for Image Pair 1</figcaption>
    </figure>


    <h3>Part B.4</h3>
    <p>
      Finally, we perform RANSAC to estimate the homographies between the two images. First, I define a helper
      function to return Hp. Next, I take a random sample of the correspondences and use those to compute a rough
      estimate of H. Then, using the helper function we solve the equation p' = Hp for p'. We can compare the output of
      this equation to our actual points in the second image to compute an error (distance) and determine which of the 
      sample points have a low enough error to be considered inliers. Then, we iterate and keep the largest set of possible 
      inliers each iteration.

      Once that's done and we have a set of possible inliers, we use those to recompute a best estimate for H.

      After this is done, I use my functions from Part A as normal, this time using the computed estimate of H to obtain the 
      following:
    </p>

    <figure>
      <img src="./media/B_automosaic1.png", width="600">
      <figcaption>Autostitched</figcaption>
    </figure>
    <figure>
      <img src="./media/mosaic1.png", width="600">
      <figcaption>Manual</figcaption>
    </figure>

    <figure>
      <img src="./media/B_automosaic2.png", width="600">
      <figcaption>Autostitched</figcaption>
    </figure>
    <figure>
      <img src="./media/mosaic2.png", width="600">
      <figcaption>Manual</figcaption>
    </figure>

    <figure>
      <img src="./media/B_automosaic3.png", width="600">
      <figcaption>Autostitched</figcaption>
    </figure>
    <figure>
      <img src="./media/mosaic3.png", width="600">
      <figcaption>Manual</figcaption>
    </figure>

</body>